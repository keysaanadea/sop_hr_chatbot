"""
CLEAN OPTIMIZED HYBRID PDF INGESTION
Rule-based detection + LLM fallback = Same quality, Much faster
‚úÖ All unused variables removed
‚úÖ Content stats properly utilized
"""

import os
import re
import pdfplumber
from datetime import datetime
from typing import List, Dict, Tuple
import time

from pinecone import Pinecone
from langchain_openai import OpenAIEmbeddings
from openai import OpenAI

from app.config import (
    OPENAI_API_KEY,
    PINECONE_API_KEY,
    PINECONE_INDEX,
    EMBEDDING_MODEL,
)

# Initialize clients
client = OpenAI(api_key=OPENAI_API_KEY)

# CLEAN CONFIG - removed unused CHUNK_SIZE and BATCH_SIZE
PDF_DIR = "documents"
CATEGORY = "sop"
PAGES_PER_BATCH = 3
TODAY = datetime.now().strftime("%Y-%m-%d")

# OPTIMIZATION TRACKING
STATS = {
    'llm_calls_saved': 0,
    'rule_based_success': 0,
    'llm_fallback_used': 0,
    'flowchart_auto': 0,
    'table_auto': 0
}

# =====================
# OPTIMIZED RULE-BASED TABLE DETECTION
# =====================
def detect_table_type_with_confidence(table_text: str) -> Tuple[Dict, float]:
    """
    Rule-based table detection with confidence score
    Returns: (parsed_data, confidence_score)
    """
    if not table_text or len(table_text.strip()) < 20:
        return {}, 0.0
    
    text_lower = table_text.lower()
    
    # PATTERN 1: Travel Rate Tables (High Confidence)
    travel_confidence = 0.0
    travel_data = []
    
    # Strong indicators for travel rates
    travel_indicators = [
        bool(re.search(r'\b(america|austria|australia|singapore|malaysia|thailand)\b', text_lower)),
        bool(re.search(r'\b(washington|vienna|sydney|kuala|bangkok)\b', text_lower)),
        len(re.findall(r'\b\d{2,3}\b', table_text)) >= 6,  # Multiple 2-3 digit numbers
        bool(re.search(r'\b(band|usd|dollar)\b', text_lower)),
        table_text.count('|') >= 3  # Table structure
    ]
    
    travel_confidence = sum(travel_indicators) / len(travel_indicators)
    
    if travel_confidence >= 0.6:
        # Extract travel rate patterns
        travel_pattern = r'([A-Z][a-zA-Z\s]+)\s*\|\s*([A-Z][a-zA-Z\s]+)\s*\|\s*(\d{2,3})\s*\|\s*(\d{2,3})\s*\|\s*(\d{2,3})'
        matches = re.finditer(travel_pattern, table_text)
        
        for match in matches:
            country = match.group(1).strip()
            city = match.group(2).strip()
            rate1, rate2, rate3 = match.group(3), match.group(4), match.group(5)
            
            # Validate rates are in expected range
            try:
                r1, r2, r3 = int(rate1), int(rate2), int(rate3)
                if all(100 <= r <= 600 for r in [r1, r2, r3]):
                    travel_data.append({
                        'type': 'travel_rate',
                        'country': country,
                        'city': city,
                        'band1': r1,
                        'band2': r2,
                        'band3': r3,
                        'formatted': f"BIAYA_PERJALANAN: {country} ({city}) - Band 1: {rate1} USD, Band 2: {rate2} USD, Band 3: {rate3} USD"
                    })
            except ValueError:
                continue
    
    if travel_data:
        return {'type': 'travel_rate', 'data': travel_data}, min(travel_confidence + 0.2, 1.0)
    
    # PATTERN 2: Band/Salary Tables
    band_confidence = 0.0
    band_data = []
    
    band_indicators = [
        bool(re.search(r'\bband\s*[1-9]\b', text_lower)),
        bool(re.search(r'\brp\s*\d+', text_lower)),
        bool(re.search(r'(maksimal|minimum|gaji|tunjangan)', text_lower)),
        len(re.findall(r'\d{1,3}[.,]?\d{3}[.,]?\d{3}', table_text)) >= 2  # Indonesian number format
    ]
    
    band_confidence = sum(band_indicators) / len(band_indicators)
    
    if band_confidence >= 0.6:
        # Extract band patterns
        band_pattern = r'band\s*([1-9])\s*.*?rp\s*([\d.,]+)'
        matches = re.finditer(band_pattern, text_lower)
        
        for match in matches:
            band = match.group(1)
            amount = match.group(2).replace('.', '').replace(',', '')
            
            try:
                amount_clean = ''.join(filter(str.isdigit, amount))
                if len(amount_clean) >= 6:  # At least 1 million
                    band_data.append({
                        'type': 'band_rate',
                        'band': band,
                        'amount': amount_clean,
                        'formatted': f"TARIF_BAND: Band {band} maksimal Rp {amount}"
                    })
            except:
                continue
    
    if band_data:
        return {'type': 'band_rate', 'data': band_data}, min(band_confidence + 0.1, 1.0)
    
    # PATTERN 3: General structured data
    general_confidence = 0.0
    
    general_indicators = [
        table_text.count('|') >= 2,
        len(table_text.split('\n')) >= 3,
        bool(re.search(r'\b(nama|kode|jenis|kategori)\b', text_lower))
    ]
    
    general_confidence = sum(general_indicators) / len(general_indicators)
    
    if general_confidence >= 0.5:
        return {'type': 'general_table', 'data': table_text}, general_confidence
    
    return {}, 0.0

def detect_flowchart_complexity(text: str) -> Tuple[bool, float, Dict]:
    """
    Detect flowchart and assess complexity for LLM decision
    Returns: (is_flowchart, complexity_score, extracted_elements)
    """
    if not text or len(text.strip()) < 50:
        return False, 0.0, {}
    
    text_lower = text.lower()
    
    # Enhanced flowchart indicators
    flowchart_indicators = [
        # Process flow indicators
        bool(re.search(r'\b(mulai|selesai|start|end|finish)\b', text_lower)),
        bool(re.search(r'\b(ya|tidak|yes|no)\s*[?]?', text_lower)),
        
        # Role-based workflow (like your examples)
        text.count('|') > 5 and any(role in text for role in ['Karyawan', 'Atasan', 'Unit', 'Peserta']),
        
        # Process verbs (Indonesian workflow language)
        len(re.findall(r'\b(melakukan|mengajukan|membuat|menerima|mengirim|verifikasi|konfirmasi)\b', text_lower)) >= 3,
        
        # Decision structures
        bool(re.search(r'\?\s*(ya|tidak|setuju)', text_lower)),
        
        # Document flow indicators
        bool(re.search(r'\b(dokumen|form|surat|sertifikat)\b', text_lower)) and len(re.findall(r'\b(dokumen|form|surat|sertifikat)\b', text_lower)) >= 2
    ]
    
    flowchart_score = sum(flowchart_indicators) / len(flowchart_indicators)
    
    if flowchart_score < 0.4:
        return False, flowchart_score, {}
    
    # Extract elements for complexity assessment
    elements = extract_flowchart_elements_optimized(text)
    
    # Calculate complexity
    complexity_factors = [
        len(elements.get('roles', [])),
        len(elements.get('processes', [])),
        len(elements.get('decisions', [])),
        len(text) // 200  # Text length factor
    ]
    
    complexity_score = min(sum(complexity_factors) / 20.0, 1.0)  # Normalize to 0-1
    
    return True, complexity_score, elements

def extract_flowchart_elements_optimized(text: str) -> Dict:
    """Optimized flowchart element extraction with better patterns"""
    elements = {
        'roles': set(),
        'processes': [],
        'decisions': [],
        'documents': set()
    }
    
    # Optimized role extraction
    role_patterns = [
        r'\b(Karyawan|Atasan|Unit|Peserta|Staff|Manager|Director|Admin|Booker|Bendahara)\b',
        r'\b(Ka\.\s*Unit|Knowledge Management|Training & Development|Verifikasi)\b'
    ]
    
    for pattern in role_patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            elements['roles'].add(match.group(1).strip())
    
    # Process extraction (limit to avoid noise)
    process_pattern = r'\b(Melakukan|Mengajukan|Membuat|Menerima|Mengirim|Melaksanakan|Konfirmasi|Verifikasi)\s+([^\.!?]{5,40})'
    matches = re.finditer(process_pattern, text, re.IGNORECASE)
    
    for match in matches:
        if len(elements['processes']) < 8:  # Limit to prevent noise
            process = f"{match.group(1)} {match.group(2).strip()}"
            elements['processes'].append(process)
    
    # Decision extraction
    decision_pattern = r'([^\.!]{5,30}\?)'
    matches = re.finditer(decision_pattern, text)
    
    for match in matches:
        if len(elements['decisions']) < 5:  # Limit decisions
            decision = match.group(1).strip()
            elements['decisions'].append(decision)
    
    # Document extraction
    doc_pattern = r'\b(Dokumen|Form|Surat|Sertifikat|P\-?JK|SPPD)\s*([A-Z\-\d]*)\b'
    matches = re.finditer(doc_pattern, text, re.IGNORECASE)
    
    for match in matches:
        doc = f"{match.group(1)} {match.group(2)}".strip()
        elements['documents'].add(doc)
    
    # Convert sets to lists for JSON serialization
    elements['roles'] = list(elements['roles'])
    elements['documents'] = list(elements['documents'])
    
    return elements

def structure_flowchart_smart(elements: Dict, original_text: str) -> str:
    """Smart flowchart structuring based on extracted elements"""
    structured_parts = ["=== WORKFLOW FLOWCHART ==="]
    
    # Add roles if found
    if elements['roles']:
        roles_text = ", ".join(elements['roles'][:6])  # Limit to prevent noise
        structured_parts.append(f"PIHAK_TERLIBAT: {roles_text}")
    
    # Add processes
    if elements['processes']:
        structured_parts.append("\n=== LANGKAH_PROSES ===")
        for i, process in enumerate(elements['processes'][:6], 1):
            structured_parts.append(f"STEP_{i}: {process}")
    
    # Add decisions
    if elements['decisions']:
        structured_parts.append("\n=== TITIK_KEPUTUSAN ===")
        for i, decision in enumerate(elements['decisions'][:4], 1):
            structured_parts.append(f"DECISION_{i}: {decision}")
    
    # Add documents
    if elements['documents']:
        docs_text = ", ".join(list(elements['documents'])[:5])
        structured_parts.append(f"\nDOKUMEN_DIPERLUKAN: {docs_text}")
    
    # Add condensed original context
    condensed = original_text[:400] + "..." if len(original_text) > 400 else original_text
    structured_parts.append(f"\nKONTEKS_ORIGINAL: {condensed}")
    
    return "\n".join(structured_parts)

# =====================
# CLEAN OPTIMIZED CONTENT EXTRACTION
# =====================
def extract_page_content_optimized(page, page_num):
    """Clean optimized content extraction - removed unused variables"""
    results = []
    
    try:
        # Strategy 1: Table extraction (removed unused tables_found variable)
        try:
            tables = page.extract_tables()
            if tables and any(is_good_table(t) for t in tables):
                for table in tables:
                    if is_good_table(table):
                        results.append({
                            'type': 'table',
                            'data': table,
                            'page': page_num + 1
                        })
                        print(f"      üìä Table found on page {page_num + 1}")
        except:
            pass
        
        # Strategy 2: Text extraction (OPTIMIZED - single call)
        text = page.extract_text()
        if text:
            cleaned = clean_text(text)
            
            # SINGLE flowchart detection call
            is_flowchart, complexity, elements = detect_flowchart_complexity(cleaned)
            
            if is_flowchart:
                print(f"      üìã Flowchart detected (complexity: {complexity:.2f}) on page {page_num + 1}")
                results.append({
                    'type': 'flowchart',
                    'data': cleaned,
                    'page': page_num + 1,
                    'complexity': complexity,
                    'elements': elements
                })
            
            # Check for country patterns
            country_matches = find_country_patterns_optimized(cleaned)
            if country_matches:
                print(f"      üåç {len(country_matches)} country patterns on page {page_num + 1}")
                for match in country_matches:
                    results.append({
                        'type': 'country_data',
                        'data': match,
                        'page': page_num + 1
                    })
            
            # Regular text processing (using cached flowchart result)
            if len(cleaned) > 100:
                if not country_matches and not is_flowchart:
                    results.append({
                        'type': 'text',
                        'data': cleaned,
                        'page': page_num + 1
                    })
                elif is_flowchart or country_matches:
                    # Keep as context if special content detected
                    results.append({
                        'type': 'text_context',
                        'data': cleaned,
                        'page': page_num + 1
                    })
    
    except Exception as e:
        print(f"      ‚ö†Ô∏è Page {page_num + 1} extraction error: {e}")
    
    return results

def find_country_patterns_optimized(text):
    """Optimized country pattern detection"""
    patterns = []
    
    # More efficient single regex pattern
    country_pattern = r'(\d+\.?\s*)?([A-Z][a-zA-Z]+(?:ia|is|ch|an|ce)?)\s+([A-Z][a-zA-Z]+)\s+(\d{2,3})\s+(\d{2,3})\s+(\d{2,3})'
    
    matches = re.finditer(country_pattern, text)
    for match in matches:
        country = match.group(2).strip()
        city = match.group(3).strip()
        rate1, rate2, rate3 = match.group(4), match.group(5), match.group(6)
        
        try:
            r1, r2, r3 = int(rate1), int(rate2), int(rate3)
            if all(100 <= r <= 600 for r in [r1, r2, r3]):
                # Country name normalization
                if country.lower().startswith("peranci"):
                    country = "Perancis"
                
                structured_data = f"BIAYA_PERJALANAN: {country} ({city}) - Band 1: {rate1} USD, Band 2: {rate2} USD, Band 3: {rate3} USD"
                patterns.append(structured_data)
                print(f"         üéØ Country: {country} {city} {rate1}-{rate2}-{rate3}")
        except ValueError:
            continue
    
    return patterns

# =====================
# HYBRID LLM PROCESSING
# =====================
def process_with_hybrid_approach(content):
    """Hybrid processing: Rule-based first, LLM fallback"""
    global STATS
    
    if content['type'] == 'table':
        # Step 1: Try rule-based detection
        table_text = []
        for row in content['data']:
            clean_row = [str(cell).strip() if cell else "" for cell in row]
            if any(clean_row):
                table_text.append(" | ".join(clean_row))
        
        table_str = "\n".join(table_text)
        
        if len(table_str) < 20:
            return None
        
        # Rule-based detection first
        parsed_data, confidence = detect_table_type_with_confidence(table_str)
        
        if confidence >= 0.7:
            # High confidence - use rule-based result
            STATS['rule_based_success'] += 1
            STATS['llm_calls_saved'] += 1
            STATS['table_auto'] += 1
            
            print(f"         üéØ Rule-based table detection (confidence: {confidence:.2f})")
            
            if parsed_data['type'] in ['travel_rate', 'band_rate']:
                return [item['formatted'] for item in parsed_data['data']]
            else:
                return [table_str]
        
        else:
            # Low confidence - use LLM fallback
            STATS['llm_fallback_used'] += 1
            print(f"         ü§ñ LLM fallback for table (confidence: {confidence:.2f})")
            return process_table_with_llm(table_str)
    
    elif content['type'] == 'flowchart':
        complexity = content.get('complexity', 0.0)
        elements = content.get('elements', {})
        
        # Auto-structure the flowchart
        auto_structured = structure_flowchart_smart(elements, content['data'])
        
        # Decision: Use LLM only for complex flowcharts
        if complexity > 0.6 and len(content['data']) > 800:
            # Complex flowchart - use LLM enhancement
            STATS['llm_fallback_used'] += 1
            print(f"         ü§ñ LLM enhancement for complex flowchart")
            llm_result = process_flowchart_with_llm(content['data'])
            return [f"LLM_WORKFLOW:\n{llm_result}", f"STRUCTURED_WORKFLOW:\n{auto_structured}"]
        else:
            # Simple flowchart - use rule-based only
            STATS['flowchart_auto'] += 1
            STATS['llm_calls_saved'] += 1
            print(f"         üéØ Rule-based flowchart processing")
            return [auto_structured]
    
    elif content['type'] == 'country_data':
        STATS['rule_based_success'] += 1
        return [content['data']]
    
    elif content['type'] in ['text', 'text_context']:
        return [content['data']]
    
    return None

def process_table_with_llm(table_str: str):
    """LLM table processing fallback"""
    prompt = f"""Transform this table data for document search. Follow these rules exactly:

RULES:
1. If you find Country + City + 3 numbers (travel rates), format as: "BIAYA_PERJALANAN: [Country] ([City]) - Band 1: X USD, Band 2: Y USD, Band 3: Z USD"
2. If you find Band + Rp amounts, format as: "TARIF_BAND: Band [X] maksimal Rp [amount]"
3. If unclear, keep original table format
4. One line per meaningful entry
5. Focus on extractable data

TABLE DATA:
{table_str}

FORMATTED OUTPUT:"""
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1000,
            temperature=0.1
        )
        
        result = response.choices[0].message.content.strip()
        
        entries = []
        for line in result.split('\n'):
            line = line.strip()
            if line and len(line) > 15:
                if not line.lower().startswith(('here', 'the table', 'this table', 'based on')):
                    entries.append(line)
        
        return entries if entries else [table_str]
        
    except Exception as e:
        print(f"         ‚ö†Ô∏è Table LLM failed: {e}")
        return [table_str]

def process_flowchart_with_llm(text: str):
    """LLM flowchart processing for complex cases"""
    prompt = f"""Analyze this complex workflow/flowchart and structure it for document search:

RULES:
1. Identify all ROLES/ACTORS involved
2. Extract KEY PROCESSES/STEPS in sequence  
3. Identify DECISION POINTS (yes/no questions)
4. List all DOCUMENTS mentioned
5. Create searchable workflow description

FORMAT:
WORKFLOW: [Brief description]
ROLES: [List all actors/roles]
PROCESS: [Key steps in sequence]
DECISIONS: [Decision points]
DOCUMENTS: [Required documents]

FLOWCHART CONTENT:
{text}

STRUCTURED OUTPUT:"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1200,
            temperature=0.1
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        print(f"         ‚ö†Ô∏è Flowchart LLM failed: {e}")
        return "LLM processing failed"

# =====================
# EXISTING HELPER FUNCTIONS (optimized)
# =====================
def is_good_table(table):
    """Enhanced table quality check"""
    if not table or len(table) < 2:
        return False
    
    non_empty = sum(1 for row in table for cell in row if str(cell).strip())
    total_cells = sum(len(row) for row in table)
    
    if total_cells == 0:
        return False
    
    fill_rate = non_empty / total_cells
    return non_empty >= 6 and fill_rate >= 0.3

def clean_text(text):
    """Optimized text cleaning"""
    # Combined regex operations for efficiency
    text = re.sub(r'(Formatted|Deleted|Added|Commented).*?(\n|$)', '', text, flags=re.I)
    text = re.sub(r'\[.*?\]|_{3,}', '', text)
    text = re.sub(r' +', ' ', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()

def create_enhanced_chunk(text, page, doc_type, filename, chunk_id):
    """Enhanced chunk creation with content type detection"""
    topic = doc_type.replace("_", " ").upper()
    
    # Content type detection for better metadata
    if "BIAYA_PERJALANAN:" in text:
        enriched_text = f"DOKUMEN: {topic} | FILE: {filename} | BIAYA_PERJALANAN_DINAS: {text}"
        content_type = "travel_rate"
    elif "WORKFLOW:" in text or "PIHAK_TERLIBAT:" in text:
        enriched_text = f"DOKUMEN: {topic} | FILE: {filename} | WORKFLOW_PROCESS: {text}"
        content_type = "flowchart"
    elif "TARIF_BAND:" in text:
        enriched_text = f"DOKUMEN: {topic} | FILE: {filename} | TARIF_JABATAN: {text}"
        content_type = "band_rate"
    else:
        enriched_text = f"DOKUMEN: {topic} | FILE: {filename} | KONTEN: {text}"
        content_type = "general"
    
    return {
        'text': enriched_text,
        'metadata': {
            "category": CATEGORY,
            "doc_type": doc_type,
            "source_file": filename,
            "pasal": f"Halaman {page}",
            "page": page,
            "text": text,
            "content_type": content_type
        },
        'id': chunk_id
    }

def upload_chunks_optimized(chunks, embedder, index):
    """Optimized upload with better logging and memory management"""
    if not chunks:
        return 0
    
    try:
        texts = [chunk['text'] for chunk in chunks]
        vectors = embedder.embed_documents(texts)
        
        payload = []
        for chunk, vector in zip(chunks, vectors):
            payload.append({
                "id": chunk['id'],
                "values": vector,
                "metadata": chunk['metadata']
            })
        
        index.upsert(payload)
        
        # Enhanced logging with content type breakdown
        content_types = {}
        for chunk in chunks:
            ctype = chunk['metadata'].get('content_type', 'general')
            content_types[ctype] = content_types.get(ctype, 0) + 1
        
        if content_types:
            type_summary = ", ".join([f"{k}: {v}" for k, v in content_types.items()])
            print(f"         üìä Uploaded - {type_summary}")
        
        # Cleanup
        del texts, vectors, payload
        
        return len(chunks)
        
    except Exception as e:
        print(f"         ‚ùå Upload failed: {e}")
        return 0

def infer_doc_type(filename):
    """Document type inference"""
    f = filename.lower()
    if "lembur" in f:
        return "sop_lembur"
    elif "perjalanan" in f or "dinas" in f:
        return "sop_perjalanan_dinas"
    elif "keuangan" in f:
        return "sop_keuangan"
    else:
        return "sop_lainnya"

# =====================
# CLEAN OPTIMIZED MAIN FUNCTION
# =====================
def main():
    """Clean optimized main function with proper content stats usage"""
    print("üöÄ CLEAN OPTIMIZED HYBRID PDF INGESTION")
    print("‚ö° Rule-based + LLM fallback = Same quality, Much faster")
    print("üßπ Cleaned: Removed unused variables, utilized content stats")
    
    try:
        pc = Pinecone(api_key=PINECONE_API_KEY)
        index = pc.Index(PINECONE_INDEX)
        embedder = OpenAIEmbeddings(model=EMBEDDING_MODEL, api_key=OPENAI_API_KEY)
        print("‚úÖ Services initialized")
    except Exception as e:
        print(f"‚ùå Service initialization failed: {e}")
        return

    if not os.path.exists(PDF_DIR):
        print(f"‚ùå Directory {PDF_DIR} not found")
        return

    total_uploaded = 0
    # Content type tracking for better analytics (now properly used)
    content_stats = {'travel_rate': 0, 'flowchart': 0, 'band_rate': 0, 'general': 0}

    for filename in os.listdir(PDF_DIR):
        if not filename.endswith(".pdf"):
            continue

        pdf_path = os.path.join(PDF_DIR, filename)
        doc_type = infer_doc_type(filename)
        
        print(f"\n{'='*60}")
        print(f"üìã FILE: {filename}")
        print(f"üìÇ TYPE: {doc_type}")
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                total_pages = len(pdf.pages)
                print(f"   üìÑ Pages: {total_pages}")
                
                chunk_counter = 0
                
                for batch_start in range(0, total_pages, PAGES_PER_BATCH):
                    batch_end = min(batch_start + PAGES_PER_BATCH, total_pages)
                    print(f"   üîÑ Processing pages {batch_start + 1}-{batch_end}")
                    
                    batch_chunks = []
                    
                    for page_num in range(batch_start, batch_end):
                        page = pdf.pages[page_num]
                        page_content = extract_page_content_optimized(page, page_num)
                        
                        for content in page_content:
                            processed = process_with_hybrid_approach(content)
                            
                            if processed:
                                for text in processed:
                                    chunk_id = f"{doc_type}_{filename}_{chunk_counter}_{TODAY}"
                                    chunk = create_enhanced_chunk(text, content['page'], doc_type, filename, chunk_id)
                                    batch_chunks.append(chunk)
                                    chunk_counter += 1
                                    
                                    # Track content types (now properly utilized)
                                    ctype = chunk['metadata']['content_type']
                                    if ctype in content_stats:
                                        content_stats[ctype] += 1
                    
                    # Optimized batch upload and memory management
                    if batch_chunks:
                        uploaded = upload_chunks_optimized(batch_chunks, embedder, index)
                        total_uploaded += uploaded
                        print(f"      ‚úÖ Batch uploaded: {uploaded} chunks (Total: {total_uploaded})")
                        
                        # Only clear batch chunks, let Python handle other GC
                        del batch_chunks
                        time.sleep(0.2)  # Reduced sleep time
        
        except Exception as e:
            print(f"‚ùå Error processing {filename}: {e}")
            continue

    # ENHANCED FINAL SUMMARY with content stats breakdown
    print(f"\n{'='*60}")
    print(f"üéâ CLEAN OPTIMIZED HYBRID INGESTION COMPLETE!")
    print(f"üìä Performance Summary:")
    print(f"   ‚Ä¢ Total chunks: {total_uploaded}")
    print(f"   ‚Ä¢ Rule-based success: {STATS['rule_based_success']}")
    print(f"   ‚Ä¢ LLM calls saved: {STATS['llm_calls_saved']}")
    print(f"   ‚Ä¢ LLM fallback used: {STATS['llm_fallback_used']}")
    print(f"   ‚Ä¢ Table auto-processed: {STATS['table_auto']}")
    print(f"   ‚Ä¢ Flowchart auto-processed: {STATS['flowchart_auto']}")
    
    # Content type breakdown (NOW PROPERLY USED)
    print(f"üìã Content Type Breakdown:")
    for content_type, count in content_stats.items():
        if count > 0:
            print(f"   ‚Ä¢ {content_type.replace('_', ' ').title()}: {count} chunks")
    
    efficiency = (STATS['llm_calls_saved'] / (STATS['llm_calls_saved'] + STATS['llm_fallback_used']) * 100) if (STATS['llm_calls_saved'] + STATS['llm_fallback_used']) > 0 else 0
    print(f"‚ö° LLM efficiency gain: {efficiency:.1f}%")
    
    print(f"üßπ Clean code: All unused variables removed, content stats utilized")
    print(f"üöÄ Hybrid processing: Rule-based + Smart LLM fallback!")

if __name__ == "__main__":
    main()